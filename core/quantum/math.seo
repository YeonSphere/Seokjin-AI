// Core Mathematical Operations - Built from scratch
module Core::AI::Math {
    use Core::Types::*
    use std::collections::*

    // Basic n-dimensional array implementation
    type NDArray {
        data: Vector<f64>,
        shape: Vector<usize>,
        strides: Vector<usize>
    }

    impl NDArray {
        // Create new array
        func new(shape: Vector<usize>) -> Self {
            let size = shape.iter().product();
            let mut strides = vec![1; shape.len()];
            
            // Calculate strides for n-dimensional access
            for i in (0..shape.len()-1).reverse() {
                strides[i] = strides[i + 1] * shape[i + 1];
            }
            
            NDArray {
                data: vec![0.0; size],
                shape,
                strides
            }
        }

        // Index calculation for n-dimensional access
        func calc_index(&self, indices: &[usize]) -> usize {
            let mut idx = 0;
            for (&dim_idx, &stride) in indices.iter().zip(self.strides.iter()) {
                idx += dim_idx * stride;
            }
            idx
        }

        // Basic mathematical operations
        func add(&self, other: &NDArray) -> Result<NDArray, Error> {
            if self.shape != other.shape {
                return Err(Error::ShapeMismatch);
            }
            
            let mut result = NDArray::new(self.shape.clone());
            for i in 0..self.data.len() {
                result.data[i] = self.data[i] + other.data[i];
            }
            Ok(result)
        }

        func multiply(&self, other: &NDArray) -> Result<NDArray, Error> {
            if self.shape != other.shape {
                return Err(Error::ShapeMismatch);
            }
            
            let mut result = NDArray::new(self.shape.clone());
            for i in 0..self.data.len() {
                result.data[i] = self.data[i] * other.data[i];
            }
            Ok(result)
        }

        // Matrix multiplication from scratch
        func matmul(&self, other: &NDArray) -> Result<NDArray, Error> {
            if self.shape.len() != 2 || other.shape.len() != 2 || 
               self.shape[1] != other.shape[0] {
                return Err(Error::ShapeMismatch);
            }
            
            let m = self.shape[0];
            let n = other.shape[1];
            let k = self.shape[1];
            
            let mut result = NDArray::new(vec![m, n]);
            
            for i in 0..m {
                for j in 0..n {
                    let mut sum = 0.0;
                    for p in 0..k {
                        sum += self.data[i * k + p] * other.data[p * n + j];
                    }
                    result.data[i * n + j] = sum;
                }
            }
            
            Ok(result)
        }

        // Activation functions
        func sigmoid(&self) -> NDArray {
            let mut result = NDArray::new(self.shape.clone());
            for i in 0..self.data.len() {
                result.data[i] = 1.0 / (1.0 + (-self.data[i]).exp());
            }
            result
        }

        func tanh(&self) -> NDArray {
            let mut result = NDArray::new(self.shape.clone());
            for i in 0..self.data.len() {
                let exp_pos = self.data[i].exp();
                let exp_neg = (-self.data[i]).exp();
                result.data[i] = (exp_pos - exp_neg) / (exp_pos + exp_neg);
            }
            result
        }

        // Gradient computation
        func gradient(&self, with_respect_to: &NDArray) -> Result<NDArray, Error> {
            if self.shape != with_respect_to.shape {
                return Err(Error::ShapeMismatch);
            }
            
            let mut result = NDArray::new(self.shape.clone());
            // Implement numerical gradient computation
            // This will be enhanced later with automatic differentiation
            let epsilon = 1e-7;
            
            for i in 0..self.data.len() {
                let orig = with_respect_to.data[i];
                
                // Forward difference approximation
                let mut forward = with_respect_to.clone();
                forward.data[i] = orig + epsilon;
                
                let mut backward = with_respect_to.clone();
                backward.data[i] = orig - epsilon;
                
                result.data[i] = (forward.data[i] - backward.data[i]) / (2.0 * epsilon);
            }
            
            Ok(result)
        }
    }
}
